## Clean Training data - remove NA and time related columns

pmlTrain<-read.csv("C:/Users/uu/Desktop/PML/Project/pml-training.csv", header=T, na.strings=c("NA", "#DIV/0!"))

View(pmlTrain)

rmNApmlTrain<-pmlTrain[, apply(pmlTrain, 2, function(x) !any(is.na(x)))] 

View(rmNApmlTrain)

## Clean Testing data - remove NA and time related columns

pmlTest<-read.csv("C:/Users/uu/Desktop/PML/Project/pml-testing.csv", header=T, na.string=c("NA", "#DIV/0!"))

View(noNApmlTest)

rmNApmlTest<-pmlTest[, apply(pmlTest, 2, function(x) !any(is.na(x)))]

View(rmNApmlTest)

## Clean variables with user information, time and undefined

cleanpmlTrain<-rmNApmlTrain[,-c(1:8)]

dim(cleanpmlTrain)

## 20 test cases with Validation data set : remove last id column

cleanpmltest<-pmlTest[,names(cleanpmlTrain[,-52])]

dim(cleanpmltest)

## Data Partitioning 

install.packages("caret")

library(caret)

inTrain<-createDataPartition(y=cleanpmlTrain$classe, p=0.60,list=False)

trainingData<-cleanpmlTrain[inTrain,]

testData<-cleanpmlTrain[-inTrain,] 

## Results and Conclusions
## Using random forest

library(caret)

set.seed(13333)

## 10 fold cross validation is used
fitControlStruct<-trainControl(method="cv", number=10, allowParallel=T, verbose=T)

randForestFit<-train(classe~.,data=trainingData, method="rf", trControl=fitControlStruct, verbose=F)

predictRF<-predict(randForestFit, newdata=testData)

confusionMatrix(predictRF, test$classe)

## Check prediction on validation set

predictionValidation<-predict(randForestFit, newdata=cleanpmltest)

predictionValidation

## Results and Conclusions
## Using LDA

ldaFit<-train(classe~.,data=trainingData, method="lda", trControl=fitControlStruct, verbose=F)

predictLDA <-predict(ldaFit, newdata=testData)

confusionMatrix(predictLDA, testData$classe)

## Decision 
## Accuracy for Random Forest model was 0.988 (95% CI: (0.9862, 0.991)) compared to 0.6895 (95% CI: (0.6792, 0.6998))
## for LDA. The expected out-of-sample error is estimated at 0.005, or 0.5%.
## The expected out-of-sample error is calculated as 1 - accuracy for predictions made against the cross-validation set.
## Test data set has 20 records so, with an accuracy about 99% on cross-validation data we can get good classification (close to 100%) ## accuracy.


